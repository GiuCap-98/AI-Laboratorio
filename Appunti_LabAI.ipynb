{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09fb70dd",
   "metadata": {},
   "source": [
    "# Appunti\n",
    "\n",
    "In questo elaborato vengono trattati i principali argomenti affrontati nella parte di laboratorio per il corso di Fondamenti di Intelligenza Artificiale presso l'Universit√† degli Studi di Verona.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86629731",
   "metadata": {},
   "source": [
    "## OpenAI\n",
    "\n",
    "Nelle lezioni di laboratorio di intelligenza artificiale √® stato utilizzato OpenAI GYM come strumento, inoltre sono stati presentati anche differenti environment:\n",
    "- **SmallMaze**\n",
    "- **LavaFloor**\n",
    "- **Cliff**\n",
    "- **CartPole**\n",
    "\n",
    "Di seguito vediamo le caratteristiche principali degli environmenti√¨, quali attributi e metodi.\n",
    "\n",
    "### Environment\n",
    "\n",
    "Un **environment** ha:\n",
    "- **variabili:\n",
    "\n",
    "    - action_space - spazio delle azioni possibili: solitamente un intervallo di numeri interi [0,...,ùëõ]\n",
    "    - Observation_space - spazio di possibili osservazioni (stati): solitamente un intervallo di numeri interi [0,...,ùëõ]\n",
    "    - actions - mappatura tra gli ID azione e le loro descrizioni\n",
    "    - startstate - start state (unico)\n",
    "    - goalstate \n",
    "    - grid - griglia appiattita (array unidimensionale)\n",
    "    - ùëá[stato_corrente, azione, stato_successore] : matrice della funzione di transizione  ùëá(ùë†,ùëé,ùë†‚Ä≤)‚Üí[0,1] (il ritorno √® una probabilit√†)\n",
    "    - ùëÖùëÜ : matrice della funzione di reward ùëÖ(ùë†)‚Üí‚Ñù (per il nostro problema il reward dipende solamente dallo stato, in quanto per ogni stato ho sempre lo stesso reward)\n",
    "\n",
    "- **metodi:\n",
    "\n",
    "    - render() - esegue il rendering dell'ambiente\n",
    "    - sample(state, action) - restituisce un nuovo stato campionato tra quelli che possono essere raggiunti da state eseguendo l'azione entrambi dati come id\n",
    "    - pos_to_state(x, y) - restituisce l'id dello stato data la sua posizione nelle coordinate ùë• e ùë¶\n",
    "    - state_to_pos(state) - restituisce le coordinate (ùë•,ùë¶) date un ID di stato\n",
    "    - step(action): esegue un azione nell'ambiente nello stato corrente. \n",
    "        **Ritorna**  (newState, reward, done) \n",
    "        **done** √® vero quando lo stato successore √® uno stato terminale.\n",
    "        **Differenza tra step e sample** con step l'azione viene effettivamente eseguita, quindi si accumula il reward e si modifica lo stato.\n",
    "    - reset(): fa tornare l'environment e l'agente allo stato di partenza    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ecd5a0",
   "metadata": {},
   "source": [
    "### Node, NodeQueue  & PriorityQueue \n",
    "\n",
    "Descriviamo alcune strutture dati appositamente gi√† implementate, che siamo andati a studiare ed usare nel **lab 1 e nel lab2**...\n",
    "\n",
    "Un **Nodo** accetta i seguenti argomenti (a cui √® possibile accedere anche come variabili dopo l'inizializzazione):\n",
    "\n",
    "- state - stato incorporato nel nodo (il suo id)\n",
    "- parent - genitore Nodo del nodo corrente in costruzione (opzionale)\n",
    "- pathcost - il costo del percorso dal nodo radice a quello corrente (predefinito a 0)\n",
    "- value - valore di un nodo. Utilizzato da PriorityQueue per ordinare il suo contenuto (il valore predefinito √® 0)\n",
    "\n",
    "example:\n",
    "\n",
    "  node = Node(stato_del_nuovo_nodo, nodo padre*(optional) )\n",
    "  node.state\n",
    "\n",
    "Analizziamo **NodeQueue**, la coda FIFO, che contiene i seguenti metodi:\n",
    "- add(node) - aggiunge un nodo alla fine della coda.\n",
    "- remove() - rimuove il primo nodo dalla coda e lo restituisce.\n",
    "- is_empty() - True se l'elenco √® vuoto, False in caso contrario.\n",
    "- state in list - True se un ID di stato √® contenuto in qualche nodo della lista, False altrimenti.\n",
    "- len(queue) - restituisce la lunghezza della lista (il numero di nodi in essa contenuti).\n",
    "\n",
    "**PriorityQueue** ordina i nodi in base a Node.value, e vengono estratti dalla coda quelli con valore pi√π alto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9f7575",
   "metadata": {},
   "source": [
    "### Bucket & BucketElimination \n",
    "\n",
    "Vediamo alcune caratteristiche degli oggetti **bucket** e **bucket elimination**\n",
    "- **Bucket(variabile, soft_cnst, hard_cnst)**: implementa la struttura dati necessaria per l'eliminazione del bucket e accetta i seguenti argomenti:\n",
    "    - variable (str) - una stringa che rappresenta la variabile del bucket (letterali)\n",
    "  - soft_cnst (elenco) - i vincoli soft, un elenco di elenchi, ogni elenco √® costruito con il nome della funzione per il primo elemento, seguito dalle variabili interessate.\n",
    "   - ineq_cnst (lista) - i vincoli rigidi (solo vincoli di disuguaglianza), un elenco di elenchi, ogni elenco rappresenta la variabile interessata ai vincoli di disuguaglianza\n",
    "       \n",
    "- **BucketElimination( domain )**: implementa i metodi necessari per la bucket elimination in forma tabellare.\n",
    "   - domain(str): indica un dict con nome delle variabili come key ed una lista di valori come value\n",
    "       Metodi:\n",
    "   - add( bucket ) - metodo che aggiunge un oggetto del bucket di classe al problema.\n",
    "   - bucket_processing() - elabora tutti i bucket nell'ordine specificato (seguendo la catena di aggiunta)\n",
    "   - value_propagation() - propaga il valore in base alla procedura di eliminazione del bucket per ottenere il massimo globale del problema dato e la corrispondente assegnazione per le variabili.\n",
    "   - plot_assignment_as_graph( assegnazione, soft_eval ) - traccia il grafico colorato dopo l'assegnazione per le variabili.\n",
    "   - get_tables() - metodo get che restituisce l'elenco delle tabelle generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfddfd7",
   "metadata": {},
   "source": [
    "### Python\n",
    "\n",
    "Andiamo a riportare brevemente alcuni elementi di python che abbiamo utilizzato:\n",
    "\n",
    "- Dizionari\n",
    "- Unpacking\n",
    "- Numpy  \n",
    "- Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92072b96",
   "metadata": {},
   "source": [
    "#### Numpy\n",
    "- np.random.choice(n, m): ritorna un vettore di size m che contiene elementi casuali tra 0 ed n \n",
    "        (**n.b** noi usiamo **policy = np.random.choice(env.action_space.n, env.observation_space.n)** per inizializzare una policy randomica) \n",
    "        \n",
    "     - np.max(array[row, col])\n",
    "     - array[0, :] ci da tutta la riga 2, nell'argomento delle colonne abbiamo usato l'operatore di slice\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a061a1",
   "metadata": {},
   "source": [
    "#### Keras\n",
    "Keras √® una libreria di rete neurale open source scritta in Python.\n",
    "Progettato per consentire una rapida sperimentazione con reti neurali profonde, si concentra sull'essere intuitivo, modulare ed estensibile.\n",
    "    \n",
    "Noi usiamo una **rete neurale feed forward**:\n",
    "Vediamo come creare una feedforward neural network in keras, questo tipo di rete √® la configurazione standard per l'approccio DQN.\n",
    "1. Definiamo il **mero di layer**input, hidden, output)\n",
    "    \n",
    "    - input_layer = 3\n",
    "    - layer_size = 5\n",
    "    - output_layer = 2\n",
    "    \n",
    "    \n",
    "2. **Creiamo il modello**, in questo caso creiamo un modello sequenziale\n",
    "    \n",
    "    - model = Sequential()\n",
    "        \n",
    "3. Dopo aver creato il modello andiamo ad **aggiungere i layer** l 1¬∞ add() aggiunge il 1¬∞ input layer e il 1¬∞ hidden layer, e cos√¨ via. Notiamo, inoltre, che andiamo ad indicare anche la funzione di attivazione per ogni layer\n",
    "       \n",
    "   - model.add(Dense(layer_size, input_dim=input_layer, activation=\"relu\")) #input layer + hidden layer #1\n",
    "   - model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #2\n",
    "   - model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #3\n",
    "   - model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #4\n",
    "   - model.add(Dense(layer_size, activation=\"relu\")) #hidden layer #5\n",
    "   - model.add(Dense(output_layer, activation=\"linear\")) #output layer\n",
    "\n",
    "4. Applichiamo il modello definitivamente e clcoliamo la **loss function** \n",
    "   - model.compile(loss=\"mean_squared_error\", optimizer='adam') \n",
    "   \n",
    "5. Una volta creato il modello... Prima facciamo il **predict dell'input**\n",
    "   - input_network = [random.uniform(0, 1), random.uniform(0, 1), random.uniform(0, 1)]\n",
    "   \n",
    "    - output_network = model.predict(np.array([input_network]))\n",
    "\n",
    "6. poi si fa il fit della rete, passando l'input e l'output atteso. Per addestrare una rete in Keras dobbiamo usare la funzione fit , che prende come input:\n",
    "\n",
    "- input : l'input della rete che vogliamo addestrare\n",
    "- Expect_output : l'output di destinazione (cio√® l'output desiderato)\n",
    "- epochs : il numero di iterazioni per la backpropagation (in DQN questo valore √® sempre 1).\n",
    "\n",
    "    - input_network = [random.uniform(0, 1), random.uniform(0, 1), random.uniform(0, 1)]\n",
    "    - expected_output = [0, 0]\n",
    "    - model.fit(np.array([input_network]), np.array([expected_output]), epochs=1000, verbose=0)\n",
    "\n",
    "\n",
    "7. poi si fa di nuovo il predict che ha dei pesi differenti\n",
    "    - print(model.predict(np.array([input_network])))\n",
    "    \n",
    "**N.B.** : In keras sia il predict che il fit prendono come input non un array ma un array di array\n",
    "\n",
    "    state = np.array([0, 0, 0])\n",
    "    #model.predict(state) #will give you an error\n",
    "    \n",
    "    #facciamo un array di array\n",
    "    state = state.reshape(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf94c1b",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network\n",
    "\n",
    "Una rete di questo tipo √® composta da:\n",
    "\n",
    "- **input layer** Questo livello accetta funzionalit√† di input. Fornisce informazioni dal mondo esterno alla rete, nessun calcolo viene eseguito a questo livello, i nodi qui passano semplicemente le informazioni (caratteristiche) al livello nascosto.\n",
    "\n",
    "- **hidden layer** I nodi di questo strato non sono esposti al mondo esterno, fanno parte dell'astrazione fornita da qualsiasi rete neurale. Il livello nascosto esegue qualsiasi tipo di calcolo sulle caratteristiche immesse tramite il livello di input e trasferisce il risultato al livello di output.\n",
    "\n",
    "- **output layer** Questo livello porta le informazioni apprese dalla rete nel mondo esterno.\n",
    "\n",
    "Una volta determinato uno o pi√π **input layer**, vengono assegnati i **pesi**. \n",
    "\n",
    "Questi pesi aiutano a determinare l'importanza di una qualsiasi data variabile, con quelli pi√π grandi che contribuiscono in modo pi√π significativo all'output rispetto agli altri input. \n",
    "Tutti gli input vengono poi moltiplicati per i rispettivi pesi e quindi sommati. \n",
    "Successivamente, l'**output** viene passato attraverso una **funzione di attivazione**, che determina l'output. \n",
    "Se supera una determinata soglia, tale output attiva il nodo, passando i dati al livello successivo nella rete. \n",
    "**L'output di un nodo diventa quindi l'input del nodo successivo. Questo processo di passaggio dei dati da un livello a quello successivo definisce questa rete neurale come una rete feedforward.**\n",
    "\n",
    "Una **funzione di attivazione** deve soddisfare alcuni criteri:\n",
    "- deve avere valori di output compresi nell‚Äôintervallo {0, 1};\n",
    "- deve fornire un valore di output vicino ad 1 quando viene sufficientemente stimolata (effetto soglia), per propagare l‚Äôattivit√† all‚Äôinterno della rete (come avviene per i neuroni naturali).\n",
    "\n",
    "Definiamo inoltre la **loss function** ovvero la funzione che vogliamo minimizzare, in quanto  quantifica la differenza tra il risultato atteso e il risultato prodotto dal modello. \n",
    "Dalla funzione di perdita, possiamo ricavare i gradienti che vengono utilizzati per aggiornare i pesi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53c7850",
   "metadata": {},
   "source": [
    "# Single Lessons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447776f1",
   "metadata": {},
   "source": [
    "## Lab1\n",
    "\n",
    "In questo laboratorio familiarizziamo con Gym, usando degli algoritmi di ricerca non informata, non si hanno quindi informazioni su quanto l'agente sia o meno vicino ad un goal state. \n",
    "Prima di tutto spieghiamo alcuni concetti base:\n",
    "- Prestazioni:\n",
    "    - completezza: √à la capacit√† dell'algoritmo di trovare una soluzione al problema per cui √® stato sviluppato\n",
    "    - complessit√† temporale: Numero di nodi generati(generati e controllati durante l'espansione di un nodo)\n",
    "    - complessit√† spaziale: Massimo numero di nodi in memoria nello stesso tempo\n",
    "    \n",
    " - Tree Search vs Graph Search:\n",
    "    L'approccio **Graph Search** prevede di usare una struttura dati in pi√π -> **explored**: contiene i nodi gi√† esplorati.\n",
    "    Quindi Graph Search prevede di effettuare un controllo sui nodi in modo da aggiungerli alla frontiera solo se non appartengono gi√† n√® alla frontiera n√® alla lista dei nodi gia esplorati.\n",
    "    \n",
    "    Prestazioni: √® pi√π difficile mantenere l'ottimalit√† per graph search in quanto ha una struttura dati in pi√π, quindi pi√π memoria\n",
    "    \n",
    "    \n",
    "Passiamo ora agli algoritmi implementati:\n",
    " - **BFS**:\n",
    "     1. Inizializzo la frontiera con il 1¬∞ nodo\n",
    "     2. Estraggo dalla frontiera il nodo e lo espando\n",
    "         - Quindi per ogni azione possibile genero un nodo figlio\n",
    "         - Se il nodo figlio non √® un goal state lo aggiungo alla frontiera\n",
    "     Continuo iterativamente fin quando la frontiera non √® vuota (oppure ho uno stato di goal prima) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ef51d",
   "metadata": {},
   "source": [
    "## Lab 2\n",
    "In questo laboratorio affrontiamo problemi di ricerca informata, in questo caso l'agente ha delle informazioni riguardo a dove si trova il goal state, queste sono dette euristiche (h(n)).\n",
    "Abbiamo implementato:\n",
    "- **Greedy best first search**: prevede la stessa idea di BFS, questa volta per√≤ i nodi sono aggiunti alla frontiera includendo anche un valore (ovvero l'euristica)\n",
    "- **A*** : prevede la stessa idea di Greedy Best First Search, questa volta per√≤ i nodi sono aggiunti alla frontiera in base ad un valore dato dalla somma del costo del cammino dal nodo iniziale fino al nodo che si sta espandendo + l'euristica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c547a98",
   "metadata": {},
   "source": [
    "## Lab 3\n",
    "\n",
    "In questo laboratorio per affrontare un problema di constraint optimization abbiamo implementato l'algoritmo **bucket elimination**, questo si compone di differenti parti:\n",
    "\n",
    "1. Inizializziamo un oggetto bucket elimination\n",
    "\n",
    "    bucket_elimination = BucketElimination(problem_domains)\n",
    "\n",
    "2. **constraint partitioning**: data una variabile v, togliamo dall'insieme dei vincoli i vincoli con v ed aggiungiamoli al bucket di v\n",
    "\n",
    "    bucket_elimination = constraint_partitioning( bucket_elimination, problem_order, problem_soft_constraints, problem_hard_constraints )\n",
    "        \n",
    "    1. Creiamo degli insiemi con i vincoli soft ed hard iniziali, S ed H\n",
    "    2. Per ogni variabile v (partendo dall'ultima)\n",
    "       3. rimuovo dagli insiemi i vincoli che contengono v\n",
    "       4. Creo degli insiemi di vincoli soft e hard che contengono v, S1 ed H2\n",
    "    4. Creo un nuovo bucket con i vincoli in S1 ed H2\n",
    "\n",
    "3. **bucket processing**: processo ogni bucket\n",
    "\n",
    "     bucket_elimination.bucket_processing()\n",
    "\n",
    "4. **calcolo le tabelle**\n",
    "\n",
    "     tab = bucket_elimination.get_tables()\n",
    "\n",
    "5. **value propagation**: come ritorno ho (un dict con) gli assrgnamenti sulle variabili\n",
    "    assignment, global_maximum = bucket_elimination.value_propagation()\n",
    "\n",
    "6. **valuto i soft constraints**:\n",
    "    evaluations = evaluate_soft_constraints( assignment, problem_soft_constraints )\n",
    "\n",
    "    1. Per ogni vincolo nei soft constraints\n",
    "    2. valuto gli assegnamenti sulle variabili\n",
    "    3. E li assrgno alla funzione di valutazione del rispettivo soft contraints\n",
    "\n",
    "        n.b. i soft constraints sono della seguente forma (funzione, variabile_1, variabile_2) \n",
    "\n",
    "7. **computo la max table** restituisce il numero massimo di elementi (righe * colonne) che appaiono in una delle tabelle nell'intero processo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6911d7",
   "metadata": {},
   "source": [
    "## Lab 4\n",
    "In questo laboratorio andiamo a risolvere un problema di decisione tramite i Markov Model.\n",
    "\n",
    "Alcune conoscenze preliminari:\n",
    "- Abbiamo un modello di transizione T[s, a, s']\n",
    "- il metodo **sample(stato_corrente, azione)** rispetto al sample nel lab 1 e nel lab 2 √® **non deterministico** in quanto eseguendo 'a' da 's' non arriver√≤ sempre nello stesso stato\n",
    "- Abbiamo R[s] e non R[s,a,s'] in quanto il reward dipende solamente dallo stato, per questo nella Bellman Equation R[s] √® fuori dalla sommatoria\n",
    "- **values_to_policy** calcola la policy, prende U lo trasforma in un numpy array e poi fa l' argmax rispetto transition model\n",
    "\n",
    "**Value Iteration**: l'idea √® che inizializzo il valore dello stato, faccio l'aggiornamento fino a convergenza e poi calcolo la policy\n",
    " 1. Per ogni stato dell'ambiente\n",
    " 2. Calcolo l'azione che massimizza la sommatoria (per ogni stato successore) del modello di transizione * utilit√† dello stato successore\n",
    " 3. Faccio l'update della funzione di utilit√†\n",
    " 4. Mi fermo quando la differenza tra la vecchia e la nuova utilit√† sar√† minima, il che vuol dire che non ho molto da migliorare\n",
    " \n",
    " - **Discussione**:\n",
    "     - Value iteration garantisce la convergenza\n",
    "     - E' efficiente, per ogni stato facciamo la sommatoria su tutti gli stati successori e poi una massimizzazione sulle azioni, quindi per ogni iterazione la complessit√† √® quadratica nel numero degli stati e lineare nel numero delle azioni\n",
    "     - Poich√® negli MDP il numero degli stati √® alto \n",
    "     \n",
    "**Policy Iteration**: Inizio con una policy arbitraria e fin quando la policy non cambia pi√π calcolo l'utilit√† data la policy (faccio un passo di policy evaluation), poi assumo che il valore sia corretto e aggiorno la policy (policy improvment). L'idea √® quindi di fare una successione di policy evaluation-policy improvment\n",
    "\n",
    "**Discussione**\n",
    "Non abbiamo l'operatore max \n",
    "Value Iteration ha un numero lineare ma alto di iterazioni, invece policy iteration converge pi√π velcemente, ma ogni iterazione costa "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e282ae6",
   "metadata": {},
   "source": [
    "## Lab 5\n",
    "\n",
    "Alcune osservazioni sul problema in questione:\n",
    "\n",
    "\"In questo caso l'ambiente √® completamente deterministico, cio√® se sceglie un azione esegue quella al 100%, facciamo reinforcement learning perch√® l'agente non sa nulla sulla struttura del reward.\n",
    "Tecnicamente l'unico stato terminale √® lo stato di goal in quanto gli stati in grigio(- 100) fanno ripartire l'agente dallo stato iniziale.\"\n",
    "\n",
    "Abbiamo implementato il Q-Learning e SARSA per risolvere un problema di reinforcement learning nel cliff-environment:\n",
    "\n",
    "### Q - Learning\n",
    "- Per ogni episodio nell'environment\n",
    "    - Inizializzo lo stato corrente come lo stato iniziale dell'environment\n",
    "    - E fintanto che non siamo in uno stato terminale\n",
    "        1. Seleziono un azione in base alla funzione di esplorazione\n",
    "        2. eseguo questa azione nello stato corrente\n",
    "        3. Seleziono l'azione che massimizza la q-function nello stato successore\n",
    "        4. Aggiorno la q-table dello stato in base alla differenza tra i nuovi valori scontati ed i vecchi valori\n",
    "        5. aggiorno i reward, la lunghezza dell'episodio e pongo lo stato corrente = stato successore, cos√¨ che continuo iterativamente\n",
    "        \n",
    "### SARSA\n",
    "- Per ogni episodio nell'environment\n",
    "    - Inizializzo lo stato corrente come lo stato iniziale dell'environment\n",
    "    - Seleziono un azione in base alla funzione di esplorazione\n",
    "    - E fintanto che non siamo in uno stato terminale\n",
    "        1. eseguo questa azione nello stato corrente\n",
    "        2. Seleziono un azione a' in base alla funzione di esplorazione nello stato successore\n",
    "        3. Aggiorno la q-table dello stato in base alla differenza tra i nuovi valori scontati ed i vecchi valori\n",
    "        4. aggiorno i reward, la lunghezza dell'episodio e pongo lo stato corrente = stato successore, cos√¨ che continuo iterativamente, ed aggiorno anche l'azione, cos√¨ che nella prossima iterazione eseguir√≤ a'\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b506ea",
   "metadata": {},
   "source": [
    "## Lab 6\n",
    "\n",
    "In questa parte di laboratorio affrontiamo un problema di Deep RL, quindi rappresentiamo la q-function con una rete neurale.\n",
    "\n",
    "In questo problema le features dello stato sono continue, inoltre non conosciamo la dinamica delle azioni, ma la rete dovr√† impararlo.\n",
    "Quando l'asta non √® i√π in equilibrio l'episodio termina\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
